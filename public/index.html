<!DOCTYPE html>
<html lang="nl">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Cyber-Agent v1.0 | Agentic UI</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <link href="https://fonts.googleapis.com/css2?family=Space+Mono&display=swap" rel="stylesheet">
    <style>
        body {
            background: #050505;
            font-family: 'Space Mono', monospace;
            overflow: hidden;
        }

        .neon-text {
            text-shadow: 0 0 10px #00ffff, 0 0 20px #00ffff;
        }

        .glitch-border {
            border: 1px solid #ff00ff;
            box-shadow: 0 0 15px rgba(255, 0, 255, 0.3);
        }

        #canvas-overlay {
            pointer-events: none;
            position: fixed;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            background: repeating-linear-gradient(0deg, rgba(0, 0, 0, 0.15) 0px, rgba(0, 0, 0, 0.15) 1px, transparent 1px, transparent 2px);
        }
    </style>
</head>

<body class="flex flex-col items-center justify-center min-h-screen p-4 text-cyan-400">

    <div id="canvas-overlay"></div>

    <!-- Agent Interface -->
    <div class="relative z-10 flex flex-col items-center">
        <svg width="300" height="300" viewBox="0 0 200 200" id="agent-svg">
            <defs>
                <filter id="glow">
                    <feGaussianBlur stdDeviation="2.5" result="coloredBlur" />
                    <feMerge>
                        <feMergeNode in="coloredBlur" />
                        <feMergeNode in="SourceGraphic" />
                    </feMerge>
                </filter>
            </defs>

            <!-- Hexagonal Frame -->
            <path id="frame" d="M 100 20 L 170 50 L 170 150 L 100 180 L 30 150 L 30 50 Z" fill="#0a0a0a"
                stroke="#00ffff" stroke-width="2" filter="url(#glow)" />

            <!-- Eyes -->
            <g id="eyes" filter="url(#glow)">
                <rect id="eye-l" x="65" y="85" width="25" height="4" fill="#ff00ff" />
                <rect id="eye-r" x="110" y="85" width="25" height="4" fill="#ff00ff" />
            </g>

            <!-- Lip-Sync Mouth -->
            <path id="mouth" d="M 70 135 L 130 135" stroke="#00ffff" stroke-width="3" fill="none" stroke-linecap="round"
                filter="url(#glow)" />
        </svg>

        <div class="mt-8 space-y-4 text-center">
            <h1 class="neon-text text-xl font-bold tracking-[0.3em]">CORE_AGENT_LINK</h1>
            <p id="status" class="text-[10px] text-magenta-500 opacity-70 italic uppercase">Status: Awaiting
                Initialization...</p>

            <input type="text" id="input-field" name="command" placeholder="COMMAND_INPUT_"
                class="glitch-border bg-transparent p-3 w-72 text-center text-cyan-400 focus:outline-none placeholder-cyan-900 transition-all">
        </div>
        <button id="start-btn" onclick="initCore()"
            class="mt-8 px-10 py-3 bg-cyan-900/20 border border-cyan-400 hover:bg-cyan-400 hover:text-black transition-all font-bold tracking-widest text-xs">
            INIT_SYSTEM_VOICE
        </button>
        <button id="mic-btn" title="Toggle microphone"
            class="ml-3 mt-8 px-3 py-2 bg-black/20 border border-cyan-400 text-xs">üé§</button>

        <div class="mt-4 w-80 text-left">
            <div id="transcript" class="glitch-border bg-black/30 p-3 h-36 overflow-auto text-sm text-cyan-200"></div>
            <div class="mt-2">
                <label class="text-xs text-cyan-300">System instructions (editable):</label>
                <textarea id="instruction-editor" rows="3"
                    class="w-full mt-1 p-2 text-xs bg-black/50 text-cyan-200 glitch-border"
                    placeholder="Edit instructions for testing..."></textarea>
                <div class="flex items-center gap-2 mt-1">
                    <input id="use-custom" type="checkbox" />
                    <label for="use-custom" class="text-xs text-cyan-300">Use custom instruction</label>
                </div>
            </div>
            <div class="mt-2 flex gap-2 justify-center">
                <button id="disconnect-btn"
                    class="px-3 py-1 bg-red-700/20 border border-red-500 text-xs">Disconnect</button>
                <button id="mute-btn" class="px-3 py-1 bg-yellow-700/20 border border-yellow-500 text-xs">Mute</button>
                <button id="reconnect-btn"
                    class="px-3 py-1 bg-green-700/20 border border-green-500 text-xs">Reconnect</button>
            </div>
        </div>
    </div>

    <script>
        const mouth = document.getElementById('mouth');
        const status = document.getElementById('status');
        const input = document.getElementById('input-field');

        // --- NIEUW: Realtime AI Koppeling ---
        // global nextStartTime so playAndVisualize can schedule audio chunks
        let nextStartTime = 0;

        // WS/audio globals
        let ws = null;
        let audioCtx = null;
        let analyser = null;
        let gainNode = null;
        let muted = false;

        async function initCore() {
            status.innerText = "Requesting_Ephemeral_Token...";

            try {
                // 1. Haal het beveiligde token op bij je Vercel API
                const startBtn = document.getElementById('start-btn');
                startBtn.disabled = true;

                const sessionResponse = await fetch("/api/session", { method: "POST" });
                if (!sessionResponse.ok) {
                    const txt = await sessionResponse.text();
                    status.innerText = "Error: Session endpoint failed";
                    console.error('session error', sessionResponse.status, txt);
                    startBtn.disabled = false;
                    return;
                }

                const sessionData = await sessionResponse.json();
                // sessionData should include client_secret.value when creating ephemeral keys
                const EPHEMERAL_KEY = sessionData?.client_secret?.value;

                // populate instruction editor with server-provided instructions for quick editing
                try {
                    const editor = document.getElementById('instruction-editor');
                    if (editor) editor.value = sessionData.instructions || editor.value || '';
                } catch (e) { /* ignore */ }

                if (!EPHEMERAL_KEY) {
                    status.innerText = "Error: No ephemeral key returned from /api/session";
                    console.error('session response', sessionData);
                    startBtn.disabled = false;
                    return;
                }

                // persist session data for later (user inputs)
                window._sessionData = sessionData;

                // Optionally show expiry if present
                if (sessionData?.client_secret?.expires_at) {
                    const expires = new Date(sessionData.client_secret.expires_at * 1000);
                    status.innerText = `Ephemeral key acquired (expires ${expires.toLocaleTimeString()})`;
                }

                // 2. Start de WebSocket
                ws = new WebSocket(
                    "wss://api.openai.com/v1/realtime",
                    ["realtime", "openai-insecure-api-key." + EPHEMERAL_KEY, "openai-beta.realtime-v1"]
                );

                // 3. Audio Context voor AI-stem
                audioCtx = new (window.AudioContext || window.webkitAudioContext)();
                analyser = audioCtx.createAnalyser();
                analyser.fftSize = 256;
                gainNode = audioCtx.createGain();
                analyser.connect(gainNode);
                gainNode.connect(audioCtx.destination);
                gainNode.gain.value = muted ? 0 : 1;

                ws.onopen = () => {
                    status.innerText = "System: Neural_Link_Established";
                    startBtn.classList.add('hidden');

                    // Stuur een welkomstbericht naar de AI
                    const customChecked = document.getElementById('use-custom')?.checked;
                    const editorVal = document.getElementById('instruction-editor')?.value;
                    const instructionToUse = (customChecked && editorVal) ? editorVal : (sessionData.instructions || "Je bent een cyberpunk AI-agent. Begroet de gebruiker kort en krachtig in het Nederlands.");

                    const event = {
                        type: "response.create",
                        response: {
                            modalities: ["audio", "text"],
                            instructions: instructionToUse,
                        }
                    };
                    ws.send(JSON.stringify(event));
                };

                ws.onerror = (ev) => {
                    console.error('WebSocket error', ev);
                    status.innerText = 'WebSocket error';
                    startBtn.disabled = false;
                };

                ws.onclose = (ev) => {
                    console.log('WebSocket closed', ev);
                    status.innerText = 'Connection closed';
                    startBtn.classList.remove('hidden');
                    startBtn.disabled = false;
                };
                // Luister naar de AI en beweeg de mond
                ws.onmessage = (event) => {
                    let msg;
                    try {
                        msg = JSON.parse(event.data);
                    } catch (e) {
                        console.warn('Non-JSON WS message', event.data);
                        return;
                    }

                    // Als de AI audio stuurt, moeten we dit decoderen en visualiseren
                    if (msg.type === "response.audio.delta") {
                        playAndVisualize(msg.audio);
                    }

                    // Tekstdelays of final text
                    if (msg.type === 'response.output_text.delta' || msg.type === 'response.create' || msg.type === 'response.output_text') {
                        const text = extractTextFromMessage(msg);
                        if (text) appendTranscript(text);
                    }
                };

                // Start animatie loop voor de mond
                animateMouth(analyser);

            } catch (e) {
                status.innerText = "Error: Link_Failed_Check_Logs";
                console.error(e);
            }
        }

        // Functie om de mond te laten bewegen op de AI-stem
        function animateMouth(analyser) {
            const dataArray = new Uint8Array(analyser.frequencyBinCount);

            function loop() {
                analyser.getByteFrequencyData(dataArray);
                const avg = dataArray.reduce((a, b) => a + b) / dataArray.length;
                const open = 135 + (avg / 3); // De 'Q' waarde voor de mond-curve
                mouth.setAttribute('d', `M 70 135 Q 100 ${open} 130 135`);
                requestAnimationFrame(loop);
            }
            loop();
        }

        // Helper om Base64 audio van OpenAI af te spelen
        function playAndVisualize(base64Audio) {
            if (!audioCtx || !analyser) return;
            // 1. Converteer Base64 naar een ArrayBuffer
            const binaryString = atob(base64Audio);
            const len = binaryString.length;
            const bytes = new Int16Array(len / 2);
            for (let i = 0; i < len; i += 2) {
                bytes[i / 2] = (binaryString.charCodeAt(i + 1) << 8) | binaryString.charCodeAt(i);
            }

            // 2. Converteer Int16 PCM naar Float32 (wat Web Audio nodig heeft)
            const float32Data = new Float32Array(bytes.length);
            for (let i = 0; i < bytes.length; i++) {
                float32Data[i] = bytes[i] / 32768.0;
            }

            // 3. Maak een AudioBuffer (OpenAI Realtime stuurt standaard 24kHz mono)
            const audioBuffer = audioCtx.createBuffer(1, float32Data.length, 24000);
            audioBuffer.getChannelData(0).set(float32Data);

            // 4. Maak de Audio Source en verbind met de Analyser
            const source = audioCtx.createBufferSource();
            source.buffer = audioBuffer;
            source.connect(analyser); // Dit zorgt ervoor dat de mond beweegt!
            // analyser is already connected to gainNode -> destination
            // 5. Plan het afspelen in (vloeiende overgang tussen chunks)
            const currentTime = audioCtx.currentTime;
            if (nextStartTime < currentTime) nextStartTime = currentTime;
            source.start(nextStartTime);
            nextStartTime += audioBuffer.duration;
        }

        // Transcript helper
        function appendTranscript(text, role = 'assistant') {
            let t = window._transcript || document.getElementById('transcript');
            if (!t) return;
            const p = document.createElement('div');
            p.textContent = text;
            p.className = role === 'user' ? 'text-right text-sky-300' : 'text-left text-cyan-200';
            t.appendChild(p);
            t.scrollTop = t.scrollHeight;
        }

        // Try to extract text from various message shapes
        function extractTextFromMessage(msg) {
            if (!msg) return null;
            if (msg.delta && (msg.delta.content || msg.delta.text)) return msg.delta.content || msg.delta.text;
            if (msg.output && msg.output_text) return msg.output_text;
            if (msg.text) return msg.text;
            if (msg.response && msg.response.output_text) return msg.response.output_text;
            // fallback: look for any property with string value
            for (const k of Object.keys(msg)) {
                if (typeof msg[k] === 'string' && msg[k].trim()) return msg[k];
            }
            return null;
        }

        // Controls: disconnect, reconnect, mute
        function disconnectWS() {
            try {
                if (ws) {
                    ws.close();
                    ws = null;
                }
            } catch (e) { console.warn(e); }
            status.innerText = 'Disconnected';
            const startBtn = document.getElementById('start-btn');
            startBtn.classList.remove('hidden');
            startBtn.disabled = false;
        }

        function reconnectWS() {
            disconnectWS();
            initCore();
        }

        function toggleMute() {
            muted = !muted;
            if (gainNode) gainNode.gain.value = muted ? 0 : 1;
            const btn = document.getElementById('mute-btn');
            if (btn) btn.textContent = muted ? 'Unmute' : 'Mute';
        }

        // Send user text input to the Realtime API via websocket
        function sendUserInput(text) {
            if (!ws || ws.readyState !== 1) {
                status.innerText = 'Not connected';
                return;
            }
            if (!text || !text.trim()) return;
            // Show user in transcript
            appendTranscript(text, 'user');

            const sessionInstructions = window._sessionData?.instructions;
            const event = {
                type: 'response.create',
                response: {
                    modalities: ['audio', 'text'],
                    instructions: sessionInstructions || undefined,
                    input: { text },
                }
            };
            ws.send(JSON.stringify(event));
        }

        // Wire input field Enter to send
        try {
            const inputEl = document.getElementById('input-field');
            if (inputEl) {
                inputEl.addEventListener('keydown', (ev) => {
                    if (ev.key === 'Enter') {
                        ev.preventDefault();
                        const v = inputEl.value;
                        inputEl.value = '';
                        sendUserInput(v);
                    }
                });
            }
        } catch (e) { console.warn(e); }

        // Simple SpeechRecognition integration (if available)
        let recognition = null;
        let listening = false;
        function toggleMic() {
            if (!('webkitSpeechRecognition' in window) && !('SpeechRecognition' in window)) {
                alert('SpeechRecognition not supported in this browser');
                return;
            }
            const SR = window.SpeechRecognition || window.webkitSpeechRecognition;
            if (!recognition) {
                recognition = new SR();
                recognition.lang = 'nl-NL';
                recognition.interimResults = false;
                recognition.maxAlternatives = 1;
                recognition.onresult = (e) => {
                    const text = e.results[0][0].transcript;
                    appendTranscript(text, 'user');
                    sendUserInput(text);
                };
                recognition.onerror = (e) => console.warn('SpeechRecognition error', e);
                recognition.onend = () => { listening = false; updateMicUI(); };
            }
            if (!listening) {
                recognition.start();
                listening = true;
            } else {
                recognition.stop();
                listening = false;
            }
            updateMicUI();
        }

        function updateMicUI() {
            const mic = document.getElementById('mic-btn');
            if (!mic) return;
            mic.textContent = listening ? 'üéôÔ∏è' : 'üé§';
            mic.classList.toggle('bg-red-600/30', listening);
        }

        try {
            document.getElementById('mic-btn')?.addEventListener('click', toggleMic);
        } catch (e) { }

        // Wire control buttons
        try {
            const d = document.getElementById('disconnect-btn');
            const r = document.getElementById('reconnect-btn');
            const m = document.getElementById('mute-btn');
            if (d) d.addEventListener('click', disconnectWS);
            if (r) r.addEventListener('click', reconnectWS);
            if (m) m.addEventListener('click', toggleMute);
        } catch (e) {
            console.warn('Failed to wire control buttons', e);
        }
    </script>
</body>

</html>