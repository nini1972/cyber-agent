<!DOCTYPE html>
<html lang="nl">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Cyber-Agent v1.0 | Agentic UI</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <style>
        @import url('https://fonts.googleapis.com');

        body {
            background: #050505;
            font-family: 'Space Mono', monospace;
            overflow: hidden;
        }

        .neon-text {
            text-shadow: 0 0 10px #00ffff, 0 0 20px #00ffff;
        }

        .glitch-border {
            border: 1px solid #ff00ff;
            box-shadow: 0 0 15px rgba(255, 0, 255, 0.3);
        }

        #canvas-overlay {
            pointer-events: none;
            position: fixed;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            background: repeating-linear-gradient(0deg, rgba(0, 0, 0, 0.15) 0px, rgba(0, 0, 0, 0.15) 1px, transparent 1px, transparent 2px);
        }
    </style>
</head>

<body class="flex flex-col items-center justify-center min-h-screen p-4 text-cyan-400">

    <div id="canvas-overlay"></div>

    <!-- Agent Interface -->
    <div class="relative z-10 flex flex-col items-center">
        <svg width="300" height="300" viewBox="0 0 200 200" id="agent-svg">
            <defs>
                <filter id="glow">
                    <feGaussianBlur stdDeviation="2.5" result="coloredBlur" />
                    <feMerge>
                        <feMergeNode in="coloredBlur" />
                        <feMergeNode in="SourceGraphic" />
                    </feMerge>
                </filter>
            </defs>

            <!-- Hexagonal Frame -->
            <path id="frame" d="M 100 20 L 170 50 L 170 150 L 100 180 L 30 150 L 30 50 Z" fill="#0a0a0a"
                stroke="#00ffff" stroke-width="2" filter="url(#glow)" />

            <!-- Eyes -->
            <g id="eyes" filter="url(#glow)">
                <rect id="eye-l" x="65" y="85" width="25" height="4" fill="#ff00ff" />
                <rect id="eye-r" x="110" y="85" width="25" height="4" fill="#ff00ff" />
            </g>

            <!-- Lip-Sync Mouth -->
            <path id="mouth" d="M 70 135 L 130 135" stroke="#00ffff" stroke-width="3" fill="none" stroke-linecap="round"
                filter="url(#glow)" />
        </svg>

        <div class="mt-8 space-y-4 text-center">
            <h1 class="neon-text text-xl font-bold tracking-[0.3em]">CORE_AGENT_LINK</h1>
            <p id="status" class="text-[10px] text-magenta-500 opacity-70 italic uppercase">Status: Awaiting
                Initialization...</p>

            <input type="text" id="input-field" placeholder="COMMAND_INPUT_"
                class="glitch-border bg-transparent p-3 w-72 text-center text-cyan-400 focus:outline-none placeholder-cyan-900 transition-all">
        </div>

        <button id="start-btn" onclick="initCore()"
            class="mt-8 px-10 py-3 bg-cyan-900/20 border border-cyan-400 hover:bg-cyan-400 hover:text-black transition-all font-bold tracking-widest text-xs">
            INIT_SYSTEM_VOICE
        </button>
    </div>

    <script>
        const mouth = document.getElementById('mouth');
        const status = document.getElementById('status');
        const input = document.getElementById('input-field');

        // --- NIEUW: Realtime AI Koppeling ---
        async function initCore() {
            status.innerText = "Requesting_Ephemeral_Token...";
            let nextStartTime = 0; // Houdt bij wanneer het volgende audio-stukje moet starten

            try {
                // 1. Haal het beveiligde token op bij je Vercel API
                const sessionResponse = await fetch("/api/session", { method: "POST" });
                const sessionData = await sessionResponse.json();
                const EPHEMERAL_KEY = sessionData.client_secret.value;

                // 2. Start de WebSocket
                const ws = new WebSocket(
                    "wss://://api.openai.com",
                    ["realtime", "openai-insecure-api-key." + EPHEMERAL_KEY, "openai-beta.realtime-v1"]
                );

                // 3. Audio Context voor AI-stem
                const audioCtx = new (window.AudioContext || window.webkitAudioContext)();
                const analyser = audioCtx.createAnalyser();
                analyser.fftSize = 256;
                analyser.connect(audioCtx.destination);

                ws.onopen = () => {
                    status.innerText = "System: Neural_Link_Established";
                    document.getElementById('start-btn').classList.add('hidden');

                    // Stuur een welkomstbericht naar de AI
                    const event = {
                        type: "response.create",
                        response: {
                            modalities: ["audio", "text"],
                            instructions: "Je bent een cyberpunk AI-agent. Begroet de gebruiker kort en krachtig in het Nederlands.",
                        }
                    };
                    ws.send(JSON.stringify(event));
                };
                // Luister naar de AI en beweeg de mond
                ws.onmessage = (event) => {
                    const msg = JSON.parse(event.data);

                    // Als de AI audio stuurt, moeten we dit decoderen en visualiseren
                    if (msg.type === "response.audio.delta") {
                        playAndVisualize(msg.audio, audioCtx, analyser);
                    }
                };

                // Start animatie loop voor de mond
                animateMouth(analyser);

            } catch (e) {
                status.innerText = "Error: Link_Failed_Check_Logs";
                console.error(e);
            }
        }

        // Functie om de mond te laten bewegen op de AI-stem
        function animateMouth(analyser) {
            const dataArray = new Uint8Array(analyser.frequencyBinCount);

            function loop() {
                analyser.getByteFrequencyData(dataArray);
                const avg = dataArray.reduce((a, b) => a + b) / dataArray.length;
                const open = 135 + (avg / 3); // De 'Q' waarde voor de mond-curve
                mouth.setAttribute('d', `M 70 135 Q 100 ${open} 130 135`);
                requestAnimationFrame(loop);
            }
            loop();
        }

        // Helper om Base64 audio van OpenAI af te spelen
        function playAndVisualize(base64Audio, ctx, analyser) {
            // 1. Converteer Base64 naar een ArrayBuffer
            const binaryString = atob(base64Audio);
            const len = binaryString.length;
            const bytes = new Int16Array(len / 2);
            for (let i = 0; i < len; i += 2) {
                bytes[i / 2] = (binaryString.charCodeAt(i + 1) << 8) | binaryString.charCodeAt(i);
            }

            // 2. Converteer Int16 PCM naar Float32 (wat Web Audio nodig heeft)
            const float32Data = new Float32Array(bytes.length);
            for (let i = 0; i < bytes.length; i++) {
                float32Data[i] = bytes[i] / 32768.0;
            }

            // 3. Maak een AudioBuffer (OpenAI Realtime stuurt standaard 24kHz mono)
            const audioBuffer = ctx.createBuffer(1, float32Data.length, 24000);
            audioBuffer.getChannelData(0).set(float32Data);

            // 4. Maak de Audio Source en verbind met de Analyser
            const source = ctx.createBufferSource();
            source.buffer = audioBuffer;
            source.connect(analyser); // Dit zorgt ervoor dat de mond beweegt!
            analyser.connect(ctx.destination);
            // 5. Plan het afspelen in (vloeiende overgang tussen chunks)
            const currentTime = ctx.currentTime;
            if (nextStartTime < currentTime) nextStartTime = currentTime;
            source.start(nextStartTime);
            nextStartTime += audioBuffer.duration;
        }
    </script>
</body>

</html>